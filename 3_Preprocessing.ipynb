{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Author: Amitabh Chakravorty\n",
        "\n",
        "DATA PREPROCESSING - Clean and prepare datasets\n",
        "\n",
        "Based on exploration results"
      ],
      "metadata": {
        "id": "oziS9m38bV25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SETUP\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pickle\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount and navigate\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/MyDrive/cryptojacking_validation'\n",
        "os.chdir(base_path)\n",
        "\n",
        "print(\"Working directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdXqIPfsbZ4O",
        "outputId": "298fba6b-138a-42ef-bbef-37643aed95ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Working directory: /content/drive/MyDrive/cryptojacking_validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PREPROCESS DS2OS DATASET\n",
        "# ============================================================================\n",
        "\n",
        "def preprocess_ds2os():\n",
        "    \"\"\"\n",
        "    Preprocess DS2OS dataset\n",
        "    Target column: 'normality'\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PREPROCESSING DS2OS DATASET\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load data\n",
        "    print(\"\\n[1/7] Loading data...\")\n",
        "    file_path = 'data/raw/ds2os/DS2OS.csv'\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Loaded: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "\n",
        "    # Identify target\n",
        "    print(\"\\n[2/7] Separating features and target\")\n",
        "    target_col = 'normality'\n",
        "\n",
        "    X = df.drop([target_col], axis=1)\n",
        "    y = df[target_col]\n",
        "\n",
        "    print(f\"Features: {X.shape[1]} columns\")\n",
        "    print(f\"Target distribution:\")\n",
        "    print(y.value_counts())\n",
        "\n",
        "    # Handle categorical features\n",
        "    print(\"\\n[3/7] Encoding categorical features...\")\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "    print(f\"Found {len(categorical_cols)} categorical columns:\")\n",
        "    for col in categorical_cols:\n",
        "        print(f\"  - {col}\")\n",
        "\n",
        "    label_encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    print(f\"All categorical features encoded\")\n",
        "\n",
        "    # Handle missing values\n",
        "    print(\"\\n[4/7] Handling missing values...\")\n",
        "    missing_count = X.isnull().sum().sum()\n",
        "    if missing_count > 0:\n",
        "        print(f\"  Filling {missing_count} missing values with column means\")\n",
        "        X = X.fillna(X.mean())\n",
        "    else:\n",
        "        print(f\"  No missing values\")\n",
        "\n",
        "    # Encode target (binary: normal=0, attack=1)\n",
        "    print(\"\\n[5/7] Encoding target variable...\")\n",
        "    # Create binary target: 'normal' = 0, everything else = 1\n",
        "    y_binary = y.apply(lambda x: 0 if x == 'normal' else 1)\n",
        "\n",
        "    print(f\"Binary encoding:\")\n",
        "    print(f\"  Normal (0): {(y_binary == 0).sum():,}\")\n",
        "    print(f\"  Attack (1): {(y_binary == 1).sum():,}\")\n",
        "\n",
        "    # Calculate imbalance ratio\n",
        "    class_counts = np.bincount(y_binary)\n",
        "    imbalance_ratio = class_counts[1] / class_counts[0] if class_counts[0] > 0 else 0\n",
        "    print(f\"  Imbalance ratio: {imbalance_ratio:.3f}\")\n",
        "\n",
        "    # Split data\n",
        "    print(\"\\n[6/7] Splitting data (70% train, 30% test)\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_binary,\n",
        "        test_size=0.3,\n",
        "        random_state=42,\n",
        "        stratify=y_binary\n",
        "    )\n",
        "\n",
        "    print(f\"Train: {X_train.shape[0]:,} samples\")\n",
        "    print(f\"Test:  {X_test.shape[0]:,} samples\")\n",
        "\n",
        "    # Apply SMOTE if severe imbalance\n",
        "    if imbalance_ratio < 0.3 or imbalance_ratio > 3.0:\n",
        "        print(f\"\\n  Severe class imbalance detected (ratio: {imbalance_ratio:.3f})\")\n",
        "        print(f\"  Applying SMOTE to balance training data...\")\n",
        "\n",
        "        smote = SMOTE(random_state=42, k_neighbors=min(5, (y_train == 1).sum() - 1))\n",
        "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "        print(f\"  After SMOTE:\")\n",
        "        print(f\"    Normal (0): {(y_train_resampled == 0).sum():,}\")\n",
        "        print(f\"    Attack (1): {(y_train_resampled == 1).sum():,}\")\n",
        "\n",
        "        X_train = X_train_resampled\n",
        "        y_train = y_train_resampled\n",
        "\n",
        "    # Scale features\n",
        "    print(\"\\n[7/7] Scaling features\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    print(f\"Features scaled (mean=0, std=1)\")\n",
        "\n",
        "    # Save processed data\n",
        "    print(\"\\n Saving processed data\")\n",
        "    np.save('data/processed/X_train_ds2os.npy', X_train_scaled)\n",
        "    np.save('data/processed/X_test_ds2os.npy', X_test_scaled)\n",
        "    np.save('data/processed/y_train_ds2os.npy', y_train)\n",
        "    np.save('data/processed/y_test_ds2os.npy', y_test)\n",
        "\n",
        "    # Save preprocessing objects\n",
        "    with open('data/processed/scaler_ds2os.pkl', 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "    with open('data/processed/label_encoders_ds2os.pkl', 'wb') as f:\n",
        "        pickle.dump(label_encoders, f)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"DS2OS PREPROCESSING COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Final train shape: {X_train_scaled.shape}\")\n",
        "    print(f\"Final test shape:  {X_test_scaled.shape}\")\n",
        "    print(f\"Features: {X_train_scaled.shape[1]}\")\n",
        "    print(f\"Class distribution (train): Normal={np.sum(y_train==0):,}, Attack={np.sum(y_train==1):,}\")\n",
        "    print(f\"Class distribution (test):  Normal={np.sum(y_test==0):,}, Attack={np.sum(y_test==1):,}\")\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test"
      ],
      "metadata": {
        "id": "50VpHHszbgi8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PREPROCESS NSL-KDD DATASET\n",
        "# ============================================================================\n",
        "\n",
        "def preprocess_nsl_kdd():\n",
        "    \"\"\"\n",
        "    Preprocess NSL-KDD dataset\n",
        "    Files: KDDTrain+.txt and KDDTest+.txt (no headers)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PREPROCESSING NSL-KDD DATASET\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # NSL-KDD standard column names\n",
        "    columns = [\n",
        "        'duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
        "        'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
        "        'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
        "        'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
        "        'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
        "        'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
        "        'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
        "        'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
        "        'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
        "        'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
        "        'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "        'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
        "        'dst_host_srv_rerror_rate', 'label', 'difficulty'\n",
        "    ]\n",
        "\n",
        "    # Load files\n",
        "    print(\"\\n[1/6] Loading data files...\")\n",
        "    train_path = 'data/raw/nsl_kdd/KDDTrain+.txt'\n",
        "    test_path = 'data/raw/nsl_kdd/KDDTest+.txt'\n",
        "\n",
        "    df_train = pd.read_csv(train_path, names=columns)\n",
        "    df_test = pd.read_csv(test_path, names=columns)\n",
        "\n",
        "    print(f\"Train: {df_train.shape[0]:,} rows\")\n",
        "    print(f\"Test:  {df_test.shape[0]:,} rows\")\n",
        "\n",
        "    # Remove difficulty column\n",
        "    print(\"\\n[2/6] Preparing data\")\n",
        "    if 'difficulty' in df_train.columns:\n",
        "        df_train = df_train.drop(['difficulty'], axis=1)\n",
        "        df_test = df_test.drop(['difficulty'], axis=1)\n",
        "\n",
        "    # Binary classification: normal vs attack\n",
        "    print(\"\\n[3/6] Creating binary labels\")\n",
        "    print(\"Original label distribution (train):\")\n",
        "    print(df_train['label'].value_counts().head(10))\n",
        "\n",
        "    df_train['label_binary'] = df_train['label'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "    df_test['label_binary'] = df_test['label'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "\n",
        "    print(f\"\\n Binary labels created:\")\n",
        "    print(f\"  Train - Normal: {(df_train['label_binary']==0).sum():,}, Attack: {(df_train['label_binary']==1).sum():,}\")\n",
        "    print(f\"  Test  - Normal: {(df_test['label_binary']==0).sum():,}, Attack: {(df_test['label_binary']==1).sum():,}\")\n",
        "\n",
        "    # Separate features and target\n",
        "    print(\"\\n[4/6] Encoding categorical features...\")\n",
        "    X_train = df_train.drop(['label', 'label_binary'], axis=1)\n",
        "    X_test = df_test.drop(['label', 'label_binary'], axis=1)\n",
        "    y_train = df_train['label_binary'].values\n",
        "    y_test = df_test['label_binary'].values\n",
        "\n",
        "    # Encode categorical features\n",
        "    categorical_cols = ['protocol_type', 'service', 'flag']\n",
        "    label_encoders = {}\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        # Fit on combined data to ensure consistent encoding\n",
        "        combined = pd.concat([X_train[col], X_test[col]])\n",
        "        le.fit(combined)\n",
        "        X_train[col] = le.transform(X_train[col])\n",
        "        X_test[col] = le.transform(X_test[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    print(f\"Encoded {len(categorical_cols)} categorical columns\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_train = X_train.values\n",
        "    X_test = X_test.values\n",
        "\n",
        "    # Scale features\n",
        "    print(\"\\n[5/6] Scaling features\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    print(f\"Features scaled\")\n",
        "\n",
        "    # Save processed data\n",
        "    print(\"\\n[6/6] Saving processed data\")\n",
        "    np.save('data/processed/X_train_nsl_kdd.npy', X_train_scaled)\n",
        "    np.save('data/processed/X_test_nsl_kdd.npy', X_test_scaled)\n",
        "    np.save('data/processed/y_train_nsl_kdd.npy', y_train)\n",
        "    np.save('data/processed/y_test_nsl_kdd.npy', y_test)\n",
        "\n",
        "    with open('data/processed/scaler_nsl_kdd.pkl', 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "    with open('data/processed/label_encoders_nsl_kdd.pkl', 'wb') as f:\n",
        "        pickle.dump(label_encoders, f)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"NSL-KDD PREPROCESSING COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Final train shape: {X_train_scaled.shape}\")\n",
        "    print(f\"Final test shape:  {X_test_scaled.shape}\")\n",
        "    print(f\"Features: {X_train_scaled.shape[1]}\")\n",
        "    print(f\"Class distribution (train): Normal={np.sum(y_train==0):,}, Attack={np.sum(y_train==1):,}\")\n",
        "    print(f\"Class distribution (test):  Normal={np.sum(y_test==0):,}, Attack={np.sum(y_test==1):,}\")\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test"
      ],
      "metadata": {
        "id": "mg25bYUkb1Ym"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM9a4LNgqQWB",
        "outputId": "14585bff-bd32-4f01-c587-7204460bffd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING DATA PREPROCESSING\n",
            "\n",
            "\n",
            " DATASET 1: DS2OS\n",
            "\n",
            "======================================================================\n",
            "PREPROCESSING DS2OS DATASET\n",
            "======================================================================\n",
            "\n",
            "[1/7] Loading data...\n",
            "Loaded: 357,952 rows × 13 columns\n",
            "\n",
            "[2/7] Separating features and target\n",
            "Features: 12 columns\n",
            "Target distribution:\n",
            "normality\n",
            "normal                           347935\n",
            "anomalous(DoSattack)               5780\n",
            "anomalous(scan)                    1547\n",
            "anomalous(malitiousControl)         889\n",
            "anomalous(malitiousOperation)       805\n",
            "anomalous(spying)                   532\n",
            "anomalous(dataProbing)              342\n",
            "anomalous(wrongSetUp)               122\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[3/7] Encoding categorical features...\n",
            "Found 11 categorical columns:\n",
            "  - sourceID\n",
            "  - sourceAddress\n",
            "  - sourceType\n",
            "  - sourceLocation\n",
            "  - destinationServiceAddress\n",
            "  - destinationServiceType\n",
            "  - destinationLocation\n",
            "  - accessedNodeAddress\n",
            "  - accessedNodeType\n",
            "  - operation\n",
            "  - value\n",
            "All categorical features encoded\n",
            "\n",
            "[4/7] Handling missing values...\n",
            "  No missing values\n",
            "\n",
            "[5/7] Encoding target variable...\n",
            "Binary encoding:\n",
            "  Normal (0): 347,935\n",
            "  Attack (1): 10,017\n",
            "  Imbalance ratio: 0.029\n",
            "\n",
            "[6/7] Splitting data (70% train, 30% test)\n",
            "Train: 250,566 samples\n",
            "Test:  107,386 samples\n",
            "\n",
            "  Severe class imbalance detected (ratio: 0.029)\n",
            "  Applying SMOTE to balance training data...\n",
            "  After SMOTE:\n",
            "    Normal (0): 243,554\n",
            "    Attack (1): 243,554\n",
            "\n",
            "[7/7] Scaling features\n",
            "Features scaled (mean=0, std=1)\n",
            "\n",
            " Saving processed data\n",
            "\n",
            "======================================================================\n",
            "DS2OS PREPROCESSING COMPLETE\n",
            "======================================================================\n",
            "Final train shape: (487108, 12)\n",
            "Final test shape:  (107386, 12)\n",
            "Features: 12\n",
            "Class distribution (train): Normal=243,554, Attack=243,554\n",
            "Class distribution (test):  Normal=104,381, Attack=3,005\n",
            "DS2OS complete!\n",
            "\n",
            "\n",
            " DATASET 2: NSL-KDD\n",
            "\n",
            "======================================================================\n",
            "PREPROCESSING NSL-KDD DATASET\n",
            "======================================================================\n",
            "\n",
            "[1/6] Loading data files...\n",
            "Train: 125,973 rows\n",
            "Test:  22,544 rows\n",
            "\n",
            "[2/6] Preparing data\n",
            "\n",
            "[3/6] Creating binary labels\n",
            "Original label distribution (train):\n",
            "label\n",
            "normal         67343\n",
            "neptune        41214\n",
            "satan           3633\n",
            "ipsweep         3599\n",
            "portsweep       2931\n",
            "smurf           2646\n",
            "nmap            1493\n",
            "back             956\n",
            "teardrop         892\n",
            "warezclient      890\n",
            "Name: count, dtype: int64\n",
            "\n",
            " Binary labels created:\n",
            "  Train - Normal: 67,343, Attack: 58,630\n",
            "  Test  - Normal: 9,711, Attack: 12,833\n",
            "\n",
            "[4/6] Encoding categorical features...\n",
            "Encoded 3 categorical columns\n",
            "\n",
            "[5/6] Scaling features\n",
            "Features scaled\n",
            "\n",
            "[6/6] Saving processed data\n",
            "\n",
            "======================================================================\n",
            "NSL-KDD PREPROCESSING COMPLETE!\n",
            "======================================================================\n",
            "Final train shape: (125973, 41)\n",
            "Final test shape:  (22544, 41)\n",
            "Features: 41\n",
            "Class distribution (train): Normal=67,343, Attack=58,630\n",
            "Class distribution (test):  Normal=9,711, Attack=12,833\n",
            "NSL-KDD complete!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RUN PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "print(\"STARTING DATA PREPROCESSING\")\n",
        "\n",
        "# Process DS2OS\n",
        "try:\n",
        "    print(\"\\n\\n DATASET 1: DS2OS\")\n",
        "    X_train_ds2os, X_test_ds2os, y_train_ds2os, y_test_ds2os = preprocess_ds2os()\n",
        "    print(\"DS2OS complete!\")\n",
        "except Exception as e:\n",
        "    print(f\" Error preprocessing DS2OS: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Process NSL-KDD\n",
        "try:\n",
        "    print(\"\\n\\n DATASET 2: NSL-KDD\")\n",
        "    X_train_nsl, X_test_nsl, y_train_nsl, y_test_nsl = preprocess_nsl_kdd()\n",
        "    print(\"NSL-KDD complete!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error preprocessing NSL-KDD: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# List all processed files\n",
        "processed_files = [f for f in os.listdir('data/processed') if f.endswith('.npy') or f.endswith('.pkl')]\n",
        "print(f\"\\n Created {len(processed_files)} files in data/processed/:\")\n",
        "for f in sorted(processed_files):\n",
        "    size_mb = os.path.getsize(f'data/processed/{f}') / (1024**2)\n",
        "    print(f\"  {f:<45} {size_mb:>8.2f} MB\")\n",
        "\n",
        "# Create summary table\n",
        "summary_data = []\n",
        "\n",
        "for dataset in ['ds2os', 'nsl_kdd']:\n",
        "    try:\n",
        "        X_train = np.load(f'data/processed/X_train_{dataset}.npy')\n",
        "        X_test = np.load(f'data/processed/X_test_{dataset}.npy')\n",
        "        y_train = np.load(f'data/processed/y_train_{dataset}.npy')\n",
        "        y_test = np.load(f'data/processed/y_test_{dataset}.npy')\n",
        "\n",
        "        summary_data.append({\n",
        "            'Dataset': dataset.upper(),\n",
        "            'Train Samples': f\"{len(y_train):,}\",\n",
        "            'Test Samples': f\"{len(y_test):,}\",\n",
        "            'Features': X_train.shape[1],\n",
        "            'Train Normal': f\"{np.sum(y_train==0):,}\",\n",
        "            'Train Attack': f\"{np.sum(y_train==1):,}\",\n",
        "            'Test Normal': f\"{np.sum(y_test==0):,}\",\n",
        "            'Test Attack': f\"{np.sum(y_test==1):,}\"\n",
        "        })\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "if summary_data:\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\n Dataset Statistics:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # Save summary\n",
        "    summary_df.to_csv('results/preprocessing_summary.csv', index=False)\n",
        "    print(\"\\n Summary saved to: results/preprocessing_summary.csv\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"ALL PREPROCESSING COMPLETE!\")\n",
        "print(\"\\nNEXT STEP: Run Model notebook (04_Model.ipynb)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyQwu5JFv1C7",
        "outputId": "67763067-f1a7-4a27-c168-8d5239d0261a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "PREPROCESSING SUMMARY\n",
            "======================================================================\n",
            "\n",
            " Created 12 files in data/processed/:\n",
            "  X_test_ds2os.npy                                  9.83 MB\n",
            "  X_test_nsl_kdd.npy                                7.05 MB\n",
            "  X_train_ds2os.npy                                44.60 MB\n",
            "  X_train_nsl_kdd.npy                              39.41 MB\n",
            "  label_encoders_ds2os.pkl                          0.12 MB\n",
            "  label_encoders_nsl_kdd.pkl                        0.00 MB\n",
            "  scaler_ds2os.pkl                                  0.00 MB\n",
            "  scaler_nsl_kdd.pkl                                0.00 MB\n",
            "  y_test_ds2os.npy                                  0.82 MB\n",
            "  y_test_nsl_kdd.npy                                0.17 MB\n",
            "  y_train_ds2os.npy                                 3.72 MB\n",
            "  y_train_nsl_kdd.npy                               0.96 MB\n",
            "\n",
            " Dataset Statistics:\n",
            "Dataset Train Samples Test Samples  Features Train Normal Train Attack Test Normal Test Attack\n",
            "  DS2OS       487,108      107,386        12      243,554      243,554     104,381       3,005\n",
            "NSL_KDD       125,973       22,544        41       67,343       58,630       9,711      12,833\n",
            "\n",
            " Summary saved to: results/preprocessing_summary.csv\n",
            "\n",
            "\n",
            "ALL PREPROCESSING COMPLETE!\n",
            "\n",
            "NEXT STEP: Run Model notebook (04_Model.ipynb)\n"
          ]
        }
      ]
    }
  ]
}